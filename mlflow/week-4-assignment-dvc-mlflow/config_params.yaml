logistic_regression:
  penalty: "l2"
  C: 1.0
  solver: "liblinear"
  max_iter: 100
  test_size: 0.3
  random_state: 42
  sales_threshold: 15  # Threshold to classify sales as high or low

decision_tree:
  criterion: "gini"          # Options: "gini", "entropy"
  splitter: "best"           # Options: "best", "random"
  max_depth: 10              # Max depth of the tree
  min_samples_split: 2       # Minimum number of samples required to split an internal node
  min_samples_leaf: 1        # Minimum number of samples required to be at a leaf node
  max_features: null         # Number of features to consider for the best split (use "null" for all)
  random_state: 42           # Random state for reproducibility
  test_size: 0.2               # Proportion of the dataset to include in the test split
  random_state: 42             # Random state for dataset splitting
  sales_threshold: 15.0        # Threshold for sales to create the binary target variable

random_forest:
  n_estimators: 100          # Number of trees in the forest
  criterion: "gini"          # Options: "gini", "entropy"
  max_depth: 10              # Max depth of the tree
  min_samples_split: 2       # Minimum number of samples required to split an internal node
  min_samples_leaf: 1        # Minimum number of samples required to be at a leaf node
  max_features: "sqrt"       # Number of features to consider for the best split
  bootstrap: True            # Whether to use bootstrap samples when building trees
  random_state: 42           # Random state for reproducibility
  test_size: 0.2               # Proportion of the dataset to include in the test split
  random_state: 42             # Random state for dataset splitting
  sales_threshold: 15.0        # Threshold for sales to create the binary target variable

xgboost:
  n_estimators: 100            # Number of boosting rounds
  max_depth: 6                 # Maximum tree depth for base learners
  learning_rate: 0.1           # Boosting learning rate (xgboost’s “eta”)
  subsample: 0.8               # Subsample ratio of the training instances
  colsample_bytree: 0.8        # Subsample ratio of columns when constructing each tree
  colsample_bylevel: 0.8       # Subsample ratio of columns for each split, in each level
  min_child_weight: 1          # Minimum sum of instance weight needed in a child
  gamma: 0.0                   # Minimum loss reduction required to make a further partition on a leaf node
  reg_alpha: 0.0               # L1 regularization term on weights
  reg_lambda: 1.0              # L2 regularization term on weights
  objective: "binary:logistic" # Binary classification objective function
  random_state: 42             # Random state for reproducibility
  test_size: 0.2                 # Proportion of the dataset to include in the test split
  random_state: 42               # Random state for dataset splitting
  sales_threshold: 15.0          # Threshold for sales to create the binary target variable